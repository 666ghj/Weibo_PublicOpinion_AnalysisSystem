#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSentimentCrawlingæ¨¡å— - ä¸»å·¥ä½œæµç¨‹
åŸºäºBroadTopicExtractionæå–çš„è¯é¢˜è¿›è¡Œå…¨å¹³å°å…³é”®è¯çˆ¬å–
"""

import sys
import argparse
from datetime import date, datetime
from pathlib import Path
from typing import List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from keyword_manager import KeywordManager
from platform_crawler import PlatformCrawler

class DeepSentimentCrawling:
    """æ·±åº¦æƒ…æ„Ÿçˆ¬å–ä¸»å·¥ä½œæµç¨‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ·±åº¦æƒ…æ„Ÿçˆ¬å–"""
        self.keyword_manager = KeywordManager()
        self.platform_crawler = PlatformCrawler()
        self.supported_platforms = ['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu']
    
    def run_daily_crawling(self, target_date: date = None, platforms: List[str] = None, 
                          max_keywords_per_platform: int = 50, 
                          max_notes_per_platform: int = 50,
                          login_type: str = "qrcode") -> Dict:
        """
        æ‰§è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            platforms: è¦çˆ¬å–çš„å¹³å°åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æ”¯æŒçš„å¹³å°
            max_keywords_per_platform: æ¯ä¸ªå¹³å°æœ€å¤§å…³é”®è¯æ•°é‡
            max_notes_per_platform: æ¯ä¸ªå¹³å°æœ€å¤§çˆ¬å–å†…å®¹æ•°é‡
            login_type: ç™»å½•æ–¹å¼
        
        Returns:
            çˆ¬å–ç»“æœç»Ÿè®¡
        """
        if not target_date:
            target_date = date.today()
        
        if not platforms:
            platforms = self.supported_platforms
        
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {target_date} çš„æ·±åº¦æƒ…æ„Ÿçˆ¬å–ä»»åŠ¡")
        print(f"ç›®æ ‡å¹³å°: {platforms}")
        
        # 1. è·å–å…³é”®è¯æ‘˜è¦
        summary = self.keyword_manager.get_crawling_summary(target_date)
        print(f"ğŸ“Š å…³é”®è¯æ‘˜è¦: {summary}")
        
        if not summary['has_data']:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¯é¢˜æ•°æ®ï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return {"success": False, "error": "æ²¡æœ‰è¯é¢˜æ•°æ®"}
        
        # 2. è·å–å…³é”®è¯ï¼ˆä¸åˆ†é…ï¼Œæ‰€æœ‰å¹³å°ä½¿ç”¨ç›¸åŒå…³é”®è¯ï¼‰
        print(f"\nğŸ“ è·å–å…³é”®è¯...")
        keywords = self.keyword_manager.get_latest_keywords(target_date, max_keywords_per_platform)
        
        if not keywords:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å…³é”®è¯ï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return {"success": False, "error": "æ²¡æœ‰å…³é”®è¯"}
        
        print(f"   è·å–åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
        print(f"   å°†åœ¨ {len(platforms)} ä¸ªå¹³å°ä¸Šçˆ¬å–æ¯ä¸ªå…³é”®è¯")
        print(f"   æ€»çˆ¬å–ä»»åŠ¡: {len(keywords)} Ã— {len(platforms)} = {len(keywords) * len(platforms)}")
        
        # 3. æ‰§è¡Œå…¨å¹³å°å…³é”®è¯çˆ¬å–
        print(f"\nğŸ”„ å¼€å§‹å…¨å¹³å°å…³é”®è¯çˆ¬å–...")
        crawl_results = self.platform_crawler.run_multi_platform_crawl_by_keywords(
            keywords, platforms, login_type, max_notes_per_platform
        )
        
        # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "date": target_date.isoformat(),
            "summary": summary,
            "crawl_results": crawl_results,
            "success": crawl_results["successful_tasks"] > 0
        }
        
        print(f"\nâœ… æ·±åº¦æƒ…æ„Ÿçˆ¬å–ä»»åŠ¡å®Œæˆ!")
        print(f"   æ—¥æœŸ: {target_date}")
        print(f"   æˆåŠŸä»»åŠ¡: {crawl_results['successful_tasks']}/{crawl_results['total_tasks']}")
        print(f"   æ€»å…³é”®è¯: {crawl_results['total_keywords']} ä¸ª")
        print(f"   æ€»å¹³å°: {crawl_results['total_platforms']} ä¸ª")
        print(f"   æ€»å†…å®¹: {crawl_results['total_notes']} æ¡")
        
        return final_report
    
    def run_platform_crawling(self, platform: str, target_date: date = None,
                             max_keywords: int = 50, max_notes: int = 50,
                             login_type: str = "qrcode") -> Dict:
        """
        æ‰§è¡Œå•ä¸ªå¹³å°çš„çˆ¬å–ä»»åŠ¡
        
        Args:
            platform: å¹³å°åç§°
            target_date: ç›®æ ‡æ—¥æœŸ
            max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡
            max_notes: æœ€å¤§çˆ¬å–å†…å®¹æ•°é‡
            login_type: ç™»å½•æ–¹å¼
        
        Returns:
            çˆ¬å–ç»“æœ
        """
        if platform not in self.supported_platforms:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")
        
        if not target_date:
            target_date = date.today()
        
        print(f"ğŸ¯ å¼€å§‹æ‰§è¡Œ {platform} å¹³å°çš„çˆ¬å–ä»»åŠ¡ ({target_date})")
        
        # è·å–å…³é”®è¯
        keywords = self.keyword_manager.get_keywords_for_platform(
            platform, target_date, max_keywords
        )
        
        if not keywords:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° {platform} å¹³å°çš„å…³é”®è¯")
            return {"success": False, "error": "æ²¡æœ‰å…³é”®è¯"}
        
        print(f"ğŸ“ å‡†å¤‡çˆ¬å– {len(keywords)} ä¸ªå…³é”®è¯")
        
        # æ‰§è¡Œçˆ¬å–
        result = self.platform_crawler.run_crawler(
            platform, keywords, login_type, max_notes
        )
        
        return result
    
    def list_available_topics(self, days: int = 7):
        """åˆ—å‡ºæœ€è¿‘å¯ç”¨çš„è¯é¢˜"""
        print(f"ğŸ“‹ æœ€è¿‘ {days} å¤©çš„è¯é¢˜æ•°æ®:")
        
        recent_topics = self.keyword_manager.db_manager.get_recent_topics(days)
        
        if not recent_topics:
            print("   æš‚æ— è¯é¢˜æ•°æ®")
            return
        
        for topic in recent_topics:
            extract_date = topic['extract_date']
            keywords_count = len(topic.get('keywords', []))
            summary_preview = topic.get('summary', '')[:100] + "..." if len(topic.get('summary', '')) > 100 else topic.get('summary', '')
            
            print(f"   ğŸ“… {extract_date}: {keywords_count} ä¸ªå…³é”®è¯")
            print(f"      æ‘˜è¦: {summary_preview}")
            print()
    
    def show_platform_guide(self):
        """æ˜¾ç¤ºå¹³å°ä½¿ç”¨æŒ‡å—"""
        print("ğŸ”§ å¹³å°çˆ¬å–æŒ‡å—:")
        print()
        
        platform_info = {
            'xhs': 'å°çº¢ä¹¦ - ç¾å¦†ã€ç”Ÿæ´»ã€æ—¶å°šå†…å®¹ä¸ºä¸»',
            'dy': 'æŠ–éŸ³ - çŸ­è§†é¢‘ã€å¨±ä¹ã€ç”Ÿæ´»å†…å®¹',
            'ks': 'å¿«æ‰‹ - ç”Ÿæ´»ã€å¨±ä¹ã€å†œæ‘é¢˜æå†…å®¹',
            'bili': 'Bç«™ - ç§‘æŠ€ã€å­¦ä¹ ã€æ¸¸æˆã€åŠ¨æ¼«å†…å®¹',
            'wb': 'å¾®åš - çƒ­ç‚¹æ–°é—»ã€æ˜æ˜Ÿã€ç¤¾ä¼šè¯é¢˜',
            'tieba': 'ç™¾åº¦è´´å§ - å…´è¶£è®¨è®ºã€æ¸¸æˆã€å­¦ä¹ ',
            'zhihu': 'çŸ¥ä¹ - çŸ¥è¯†é—®ç­”ã€æ·±åº¦è®¨è®º'
        }
        
        for platform, desc in platform_info.items():
            print(f"   {platform}: {desc}")
        
        print()
        print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("   1. é¦–æ¬¡ä½¿ç”¨éœ€è¦æ‰«ç ç™»å½•å„å¹³å°")
        print("   2. å»ºè®®å…ˆæµ‹è¯•å•ä¸ªå¹³å°ï¼Œç¡®è®¤ç™»å½•æ­£å¸¸")
        print("   3. çˆ¬å–æ•°é‡ä¸å®œè¿‡å¤§ï¼Œé¿å…è¢«é™åˆ¶")
        print("   4. å¯ä»¥ä½¿ç”¨ --test æ¨¡å¼è¿›è¡Œå°è§„æ¨¡æµ‹è¯•")
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.keyword_manager:
            self.keyword_manager.close()

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="DeepSentimentCrawling - åŸºäºè¯é¢˜çš„æ·±åº¦æƒ…æ„Ÿçˆ¬å–")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--date", type=str, help="ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©")
    parser.add_argument("--platform", type=str, choices=['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu'], 
                       help="æŒ‡å®šå•ä¸ªå¹³å°è¿›è¡Œçˆ¬å–")
    parser.add_argument("--platforms", type=str, nargs='+', 
                       choices=['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu'],
                       help="æŒ‡å®šå¤šä¸ªå¹³å°è¿›è¡Œçˆ¬å–")
    
    # çˆ¬å–å‚æ•°
    parser.add_argument("--max-keywords", type=int, default=50, 
                       help="æ¯ä¸ªå¹³å°æœ€å¤§å…³é”®è¯æ•°é‡ (é»˜è®¤: 50)")
    parser.add_argument("--max-notes", type=int, default=50,
                       help="æ¯ä¸ªå¹³å°æœ€å¤§çˆ¬å–å†…å®¹æ•°é‡ (é»˜è®¤: 50)")
    parser.add_argument("--login-type", type=str, choices=['qrcode', 'phone', 'cookie'], 
                       default='qrcode', help="ç™»å½•æ–¹å¼ (é»˜è®¤: qrcode)")
    
    # åŠŸèƒ½å‚æ•°
    parser.add_argument("--list-topics", action="store_true", help="åˆ—å‡ºæœ€è¿‘çš„è¯é¢˜æ•°æ®")
    parser.add_argument("--days", type=int, default=7, help="æŸ¥çœ‹æœ€è¿‘å‡ å¤©çš„è¯é¢˜ (é»˜è®¤: 7)")
    parser.add_argument("--guide", action="store_true", help="æ˜¾ç¤ºå¹³å°ä½¿ç”¨æŒ‡å—")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ (å°‘é‡æ•°æ®)")
    
    args = parser.parse_args()
    
    # è§£ææ—¥æœŸ
    target_date = None
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        except ValueError:
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
            return
    
    # åˆ›å»ºçˆ¬å–å®ä¾‹
    crawler = DeepSentimentCrawling()
    
    try:
        # æ˜¾ç¤ºæŒ‡å—
        if args.guide:
            crawler.show_platform_guide()
            return
        
        # åˆ—å‡ºè¯é¢˜
        if args.list_topics:
            crawler.list_available_topics(args.days)
            return
        
        # æµ‹è¯•æ¨¡å¼è°ƒæ•´å‚æ•°
        if args.test:
            args.max_keywords = min(args.max_keywords, 10)
            args.max_notes = min(args.max_notes, 10)
            print("æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶å…³é”®è¯å’Œå†…å®¹æ•°é‡")
        
        # å•å¹³å°çˆ¬å–
        if args.platform:
            result = crawler.run_platform_crawling(
                args.platform, target_date, args.max_keywords, 
                args.max_notes, args.login_type
            )
            
            if result['success']:
                print(f"\n{args.platform} çˆ¬å–æˆåŠŸï¼")
            else:
                print(f"\n{args.platform} çˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            return
        
        # å¤šå¹³å°çˆ¬å–
        platforms = args.platforms if args.platforms else None
        result = crawler.run_daily_crawling(
            target_date, platforms, args.max_keywords, 
            args.max_notes, args.login_type
        )
        
        if result['success']:
            print(f"\nå¤šå¹³å°çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        else:
            print(f"\nå¤šå¹³å°çˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\næ‰§è¡Œå‡ºé”™: {e}")
    finally:
        crawler.close()

if __name__ == "__main__":
    main()
