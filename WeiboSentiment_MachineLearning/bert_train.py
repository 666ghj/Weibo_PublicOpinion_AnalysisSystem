# -*- coding: utf-8 -*-
"""
BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒè„šæœ¬
"""
import argparse
import os
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score
from typing import List, Tuple
import warnings
import requests
from pathlib import Path

from base_model import BaseModel
from utils import load_corpus_bert

# å¿½ç•¥transformersçš„è­¦å‘Š
warnings.filterwarnings("ignore")
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


class BertDataset(Dataset):
    """BERTæ•°æ®é›†"""
    
    def __init__(self, data: List[Tuple[str, int]]):
        self.data = [item[0] for item in data]
        self.labels = [item[1] for item in data]
    
    def __getitem__(self, index):
        return self.data[index], self.labels[index]
    
    def __len__(self):
        return len(self.labels)


class BertClassifier(nn.Module):
    """BERTåˆ†ç±»å™¨ç½‘ç»œ"""
    
    def __init__(self, input_size):
        super(BertClassifier, self).__init__()
        self.fc = nn.Linear(input_size, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        out = self.fc(x)
        out = self.sigmoid(out)
        return out


class BertModel_Custom(BaseModel):
    """BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    
    def __init__(self, model_path: str = "./model/chinese_wwm_pytorch"):
        super().__init__("BERT")
        self.model_path = model_path
        self.tokenizer = None
        self.bert = None
        self.classifier = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def _download_bert_model(self):
        """è‡ªåŠ¨ä¸‹è½½BERTé¢„è®­ç»ƒæ¨¡å‹"""
        print(f"BERTæ¨¡å‹ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è½½ä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹...")
        print("ä¸‹è½½æ¥æº: bert-base-chinese (Hugging Face)")
        
        try:
            # åˆ›å»ºæ¨¡å‹ç›®å½•
            os.makedirs(self.model_path, exist_ok=True)
            
            # ä½¿ç”¨Hugging Faceçš„ä¸­æ–‡BERTæ¨¡å‹
            model_name = "bert-base-chinese"
            print(f"æ­£åœ¨ä»Hugging Faceä¸‹è½½ {model_name}...")
            
            # ä¸‹è½½tokenizer
            print("ä¸‹è½½åˆ†è¯å™¨...")
            tokenizer = BertTokenizer.from_pretrained(model_name)
            tokenizer.save_pretrained(self.model_path)
            
            # ä¸‹è½½æ¨¡å‹
            print("ä¸‹è½½BERTæ¨¡å‹...")
            bert_model = BertModel.from_pretrained(model_name)
            bert_model.save_pretrained(self.model_path)
            
            print(f"âœ… BERTæ¨¡å‹ä¸‹è½½å®Œæˆï¼Œä¿å­˜åœ¨: {self.model_path}")
            return True
            
        except Exception as e:
            print(f"âŒ BERTæ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            print("\nğŸ’¡ æ‚¨å¯ä»¥æ‰‹åŠ¨ä¸‹è½½BERTæ¨¡å‹:")
            print("1. è®¿é—® https://huggingface.co/bert-base-chinese")
            print("2. æˆ–ä½¿ç”¨å“ˆå·¥å¤§ä¸­æ–‡BERT: https://github.com/ymcui/Chinese-BERT-wwm")
            print(f"3. å°†æ¨¡å‹æ–‡ä»¶è§£å‹åˆ°: {self.model_path}")
            return False
    
    def _load_bert(self):
        """åŠ è½½BERTæ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"åŠ è½½BERTæ¨¡å‹: {self.model_path}")
        
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½
        if not os.path.exists(self.model_path) or not any(os.scandir(self.model_path)):
            print("BERTæ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½...")
            if not self._download_bert_model():
                raise FileNotFoundError(f"BERTæ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½åˆ°: {self.model_path}")
        
        try:
            self.tokenizer = BertTokenizer.from_pretrained(self.model_path)
            self.bert = BertModel.from_pretrained(self.model_path).to(self.device)
            
            # å†»ç»“BERTå‚æ•°
            for param in self.bert.parameters():
                param.requires_grad = False
                
            print("âœ… BERTæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨åœ¨çº¿æ¨¡å‹...")
            
            # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
            try:
                model_name = "bert-base-chinese"
                self.tokenizer = BertTokenizer.from_pretrained(model_name)
                self.bert = BertModel.from_pretrained(model_name).to(self.device)
                
                # å†»ç»“BERTå‚æ•°
                for param in self.bert.parameters():
                    param.requires_grad = False
                    
                print("âœ… åœ¨çº¿BERTæ¨¡å‹åŠ è½½å®Œæˆ")
                
            except Exception as e2:
                print(f"âŒ åœ¨çº¿æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                raise FileNotFoundError(f"æ— æ³•åŠ è½½BERTæ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {self.model_path}")
    
    def train(self, train_data: List[Tuple[str, int]], **kwargs) -> None:
        """è®­ç»ƒBERTæ¨¡å‹"""
        print(f"å¼€å§‹è®­ç»ƒ {self.model_name} æ¨¡å‹...")
        
        # åŠ è½½BERT
        self._load_bert()
        
        # è¶…å‚æ•°
        learning_rate = kwargs.get('learning_rate', 1e-3)
        num_epochs = kwargs.get('num_epochs', 10)
        batch_size = kwargs.get('batch_size', 100)
        input_size = kwargs.get('input_size', 768)
        decay_rate = kwargs.get('decay_rate', 0.9)
        
        print(f"BERTè¶…å‚æ•°: lr={learning_rate}, epochs={num_epochs}, "
              f"batch_size={batch_size}, input_size={input_size}")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = BertDataset(train_data)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # åˆ›å»ºåˆ†ç±»å™¨
        self.classifier = BertClassifier(input_size).to(self.device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(self.classifier.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)
        
        # è®­ç»ƒå¾ªç¯
        self.bert.eval()  # BERTå§‹ç»ˆä¿æŒè¯„ä¼°æ¨¡å¼
        self.classifier.train()
        
        for epoch in range(num_epochs):
            total_loss = 0
            num_batches = 0
            
            for i, (words, labels) in enumerate(train_loader):
                # åˆ†è¯å’Œç¼–ç 
                tokens = self.tokenizer(words, padding=True, truncation=True, 
                                      max_length=512, return_tensors='pt')
                input_ids = tokens["input_ids"].to(self.device)
                attention_mask = tokens["attention_mask"].to(self.device)
                labels = torch.tensor(labels, dtype=torch.float32).to(self.device)
                
                # è·å–BERTè¾“å‡ºï¼ˆå†»ç»“å‚æ•°ï¼‰
                with torch.no_grad():
                    bert_outputs = self.bert(input_ids, attention_mask=attention_mask)
                    bert_output = bert_outputs[0][:, 0]  # [CLS] tokençš„è¾“å‡º
                
                # åˆ†ç±»å™¨å‰å‘ä¼ æ’­
                optimizer.zero_grad()
                outputs = self.classifier(bert_output)
                logits = outputs.view(-1)
                loss = criterion(logits, labels)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                if (i + 1) % 10 == 0:
                    avg_loss = total_loss / num_batches
                    print(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {avg_loss:.4f}")
                    total_loss = 0
                    num_batches = 0
            
            # å­¦ä¹ ç‡è¡°å‡
            scheduler.step()
            
            # ä¿å­˜æ¯ä¸ªepochçš„æ¨¡å‹
            if kwargs.get('save_each_epoch', False):
                epoch_model_path = f"./model/bert_epoch_{epoch+1}.pth"
                os.makedirs(os.path.dirname(epoch_model_path), exist_ok=True)
                torch.save(self.classifier.state_dict(), epoch_model_path)
                print(f"å·²ä¿å­˜æ¨¡å‹: {epoch_model_path}")
        
        self.is_trained = True
        print(f"{self.model_name} æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    def predict(self, texts: List[str]) -> List[int]:
        """é¢„æµ‹æ–‡æœ¬æƒ…æ„Ÿ"""
        if not self.is_trained:
            raise ValueError(f"æ¨¡å‹ {self.model_name} å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨trainæ–¹æ³•")
        
        predictions = []
        batch_size = 32
        
        self.bert.eval()
        self.classifier.eval()
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # åˆ†è¯å’Œç¼–ç 
                tokens = self.tokenizer(batch_texts, padding=True, truncation=True,
                                      max_length=512, return_tensors='pt')
                input_ids = tokens["input_ids"].to(self.device)
                attention_mask = tokens["attention_mask"].to(self.device)
                
                # è·å–BERTè¾“å‡º
                bert_outputs = self.bert(input_ids, attention_mask=attention_mask)
                bert_output = bert_outputs[0][:, 0]
                
                # åˆ†ç±»å™¨é¢„æµ‹
                outputs = self.classifier(bert_output)
                outputs = outputs.view(-1)
                
                # è½¬æ¢ä¸ºç±»åˆ«æ ‡ç­¾
                preds = (outputs > 0.5).cpu().numpy()
                predictions.extend(preds.astype(int).tolist())
        
        return predictions
    
    def predict_single(self, text: str) -> Tuple[int, float]:
        """é¢„æµ‹å•æ¡æ–‡æœ¬çš„æƒ…æ„Ÿ"""
        if not self.is_trained:
            raise ValueError(f"æ¨¡å‹ {self.model_name} å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨trainæ–¹æ³•")
        
        self.bert.eval()
        self.classifier.eval()
        
        with torch.no_grad():
            # åˆ†è¯å’Œç¼–ç 
            tokens = self.tokenizer([text], padding=True, truncation=True,
                                  max_length=512, return_tensors='pt')
            input_ids = tokens["input_ids"].to(self.device)
            attention_mask = tokens["attention_mask"].to(self.device)
            
            # è·å–BERTè¾“å‡º
            bert_outputs = self.bert(input_ids, attention_mask=attention_mask)
            bert_output = bert_outputs[0][:, 0]
            
            # åˆ†ç±»å™¨é¢„æµ‹
            output = self.classifier(bert_output)
            prob = output.item()
            
            prediction = int(prob > 0.5)
            confidence = prob if prediction == 1 else 1 - prob
        
        return prediction, confidence
    
    def save_model(self, model_path: str = None) -> None:
        """ä¿å­˜æ¨¡å‹"""
        if not self.is_trained:
            raise ValueError(f"æ¨¡å‹ {self.model_name} å°šæœªè®­ç»ƒï¼Œæ— æ³•ä¿å­˜")
        
        if model_path is None:
            model_path = f"./model/{self.model_name.lower()}_model.pth"
        
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        # ä¿å­˜åˆ†ç±»å™¨å’Œç›¸å…³ä¿¡æ¯
        model_data = {
            'classifier_state_dict': self.classifier.state_dict(),
            'model_path': self.model_path,
            'input_size': 768,
            'device': str(self.device)
        }
        
        torch.save(model_data, model_path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def load_model(self, model_path: str) -> None:
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        model_data = torch.load(model_path, map_location=self.device)
        
        # è®¾ç½®BERTæ¨¡å‹è·¯å¾„
        self.model_path = model_data['model_path']
        
        # åŠ è½½BERT
        self._load_bert()
        
        # é‡å»ºåˆ†ç±»å™¨
        input_size = model_data['input_size']
        self.classifier = BertClassifier(input_size).to(self.device)
        
        # åŠ è½½åˆ†ç±»å™¨æƒé‡
        self.classifier.load_state_dict(model_data['classifier_state_dict'])
        
        self.is_trained = True
        print(f"å·²åŠ è½½æ¨¡å‹: {model_path}")
    
    @staticmethod
    def load_data(train_path: str, test_path: str) -> Tuple[List[Tuple[str, int]], List[Tuple[str, int]]]:
        """åŠ è½½BERTæ ¼å¼çš„æ•°æ®"""
        print("åŠ è½½è®­ç»ƒæ•°æ®...")
        train_data = load_corpus_bert(train_path)
        print(f"è®­ç»ƒæ•°æ®é‡: {len(train_data)}")
        
        print("åŠ è½½æµ‹è¯•æ•°æ®...")
        test_data = load_corpus_bert(test_path)
        print(f"æµ‹è¯•æ•°æ®é‡: {len(test_data)}")
        
        return train_data, test_data


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--train_path', type=str, default='./data/weibo2018/train.txt',
                        help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--test_path', type=str, default='./data/weibo2018/test.txt',
                        help='æµ‹è¯•æ•°æ®è·¯å¾„')
    parser.add_argument('--model_path', type=str, default='./model/bert_model.pth',
                        help='æ¨¡å‹ä¿å­˜è·¯å¾„')
    parser.add_argument('--bert_path', type=str, default='./model/chinese_wwm_pytorch',
                        help='BERTé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=10,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=100,
                        help='æ‰¹å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-3,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--eval_only', action='store_true',
                        help='ä»…è¯„ä¼°å·²æœ‰æ¨¡å‹ï¼Œä¸è¿›è¡Œè®­ç»ƒ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨¡å‹
    model = BertModel_Custom(args.bert_path)
    
    if args.eval_only:
        # ä»…è¯„ä¼°æ¨¡å¼
        print("è¯„ä¼°æ¨¡å¼ï¼šåŠ è½½å·²æœ‰æ¨¡å‹è¿›è¡Œè¯„ä¼°")
        model.load_model(args.model_path)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        _, test_data = model.load_data(args.train_path, args.test_path)
        
        # è¯„ä¼°æ¨¡å‹
        model.evaluate(test_data)
    else:
        # è®­ç»ƒæ¨¡å¼
        # åŠ è½½æ•°æ®
        train_data, test_data = model.load_data(args.train_path, args.test_path)
        
        # è®­ç»ƒæ¨¡å‹
        model.train(
            train_data,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate
        )
        
        # è¯„ä¼°æ¨¡å‹
        model.evaluate(test_data)
        
        # ä¿å­˜æ¨¡å‹
        model.save_model(args.model_path)
        
        # ç¤ºä¾‹é¢„æµ‹
        print("\nç¤ºä¾‹é¢„æµ‹:")
        test_texts = [
            "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ£’",
            "è¿™éƒ¨ç”µå½±å¤ªæ— èŠäº†ï¼Œæµªè´¹æ—¶é—´",
            "å“ˆå“ˆå“ˆï¼Œå¤ªæœ‰è¶£äº†"
        ]
        
        for text in test_texts:
            pred, conf = model.predict_single(text)
            sentiment = "æ­£é¢" if pred == 1 else "è´Ÿé¢"
            print(f"æ–‡æœ¬: {text}")
            print(f"é¢„æµ‹: {sentiment} (ç½®ä¿¡åº¦: {conf:.4f})")
            print()


if __name__ == "__main__":
    main()