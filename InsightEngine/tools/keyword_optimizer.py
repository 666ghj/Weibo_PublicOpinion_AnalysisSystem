"""
å…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
ä½¿ç”¨Qwen AIå°†Agentç”Ÿæˆçš„æœç´¢è¯ä¼˜åŒ–ä¸ºæ›´é€‚åˆèˆ†æƒ…æ•°æ®åº“æŸ¥è¯¢çš„å…³é”®è¯
"""

from openai import OpenAI
import json
import sys
import os
from typing import List, Dict, Any
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ä»¥å¯¼å…¥config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from config import KEYWORD_OPTIMIZER_API_KEY, KEYWORD_OPTIMIZER_BASE_URL, KEYWORD_OPTIMIZER_MODEL_NAME

# æ·»åŠ utilsç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(os.path.dirname(current_dir))
utils_dir = os.path.join(root_dir, 'utils')
if utils_dir not in sys.path:
    sys.path.append(utils_dir)

from retry_helper import with_graceful_retry, SEARCH_API_RETRY_CONFIG

@dataclass
class KeywordOptimizationResponse:
    """å…³é”®è¯ä¼˜åŒ–å“åº”"""
    original_query: str
    optimized_keywords: List[str]
    reasoning: str
    success: bool
    error_message: str = ""

class KeywordOptimizer:
    """
    å…³é”®è¯ä¼˜åŒ–å™¨
    ä½¿ç”¨ç¡…åŸºæµåŠ¨çš„Qwen3æ¨¡å‹å°†Agentç”Ÿæˆçš„æœç´¢è¯ä¼˜åŒ–ä¸ºæ›´è´´è¿‘çœŸå®èˆ†æƒ…çš„å…³é”®è¯
    """
    
    def __init__(self, api_key: str = None, base_url: str = None, model_name: str = None):
        """
        åˆå§‹åŒ–å…³é”®è¯ä¼˜åŒ–å™¨
        
        Args:
            api_key: ç¡…åŸºæµåŠ¨APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®æ–‡ä»¶è¯»å–
            base_url: æ¥å£åŸºç¡€åœ°å€ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶æä¾›çš„SiliconFlowåœ°å€
        """
        self.api_key = api_key or KEYWORD_OPTIMIZER_API_KEY

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°ç¡…åŸºæµåŠ¨APIå¯†é’¥ï¼Œè¯·åœ¨config.pyä¸­è®¾ç½®KEYWORD_OPTIMIZER_API_KEY")

        self.base_url = base_url or KEYWORD_OPTIMIZER_BASE_URL

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        self.model = model_name or KEYWORD_OPTIMIZER_MODEL_NAME
    
    def optimize_keywords(self, original_query: str, context: str = "") -> KeywordOptimizationResponse:
        """
        ä¼˜åŒ–æœç´¢å…³é”®è¯
        
        Args:
            original_query: Agentç”Ÿæˆçš„åŸå§‹æœç´¢æŸ¥è¯¢
            context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æ®µè½æ ‡é¢˜ã€å†…å®¹æè¿°ç­‰ï¼‰
            
        Returns:
            KeywordOptimizationResponse: ä¼˜åŒ–åçš„å…³é”®è¯åˆ—è¡¨
        """
        print(f"ğŸ” å…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶: å¤„ç†æŸ¥è¯¢ '{original_query}'")
        
        try:
            # æ„å»ºä¼˜åŒ–prompt
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(original_query, context)
            
            # è°ƒç”¨Qwen API
            response = self._call_qwen_api(system_prompt, user_prompt)
            
            if response["success"]:
                # è§£æå“åº”
                content = response["content"]
                try:
                    # å°è¯•è§£æJSONæ ¼å¼çš„å“åº”
                    if content.strip().startswith('{'):
                        parsed = json.loads(content)
                        keywords = parsed.get("keywords", [])
                        reasoning = parsed.get("reasoning", "")
                    else:
                        # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
                        keywords = self._extract_keywords_from_text(content)
                        reasoning = content
                    
                    # éªŒè¯å…³é”®è¯è´¨é‡
                    validated_keywords = self._validate_keywords(keywords)
                    
                    print(f"âœ… ä¼˜åŒ–æˆåŠŸ: {len(validated_keywords)}ä¸ªå…³é”®è¯")
                    for i, keyword in enumerate(validated_keywords, 1):
                        print(f"   {i}. '{keyword}'")
                    
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=validated_keywords,
                        reasoning=reasoning,
                        success=True
                    )
                
                except Exception as e:
                    print(f"âš ï¸ è§£æå“åº”å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {str(e)}")
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä»åŸå§‹æŸ¥è¯¢ä¸­æå–å…³é”®è¯
                    fallback_keywords = self._fallback_keyword_extraction(original_query)
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=fallback_keywords,
                        reasoning="APIå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–",
                        success=True
                    )
            else:
                print(f"âŒ APIè°ƒç”¨å¤±è´¥: {response['error']}")
                # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                fallback_keywords = self._fallback_keyword_extraction(original_query)
                return KeywordOptimizationResponse(
                    original_query=original_query,
                    optimized_keywords=fallback_keywords,
                    reasoning="APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–",
                    success=True,
                    error_message=response['error']
                )
                
        except Exception as e:
            print(f"âŒ å…³é”®è¯ä¼˜åŒ–å¤±è´¥: {str(e)}")
            # æœ€ç»ˆå¤‡ç”¨æ–¹æ¡ˆ
            fallback_keywords = self._fallback_keyword_extraction(original_query)
            return KeywordOptimizationResponse(
                original_query=original_query,
                optimized_keywords=fallback_keywords,
                reasoning="ç³»ç»Ÿé”™è¯¯ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–",
                success=False,
                error_message=str(e)
            )
    
    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿprompt"""
        return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„èˆ†æƒ…æ•°æ®æŒ–æ˜ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æä¾›çš„æœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸ºæ›´é€‚åˆåœ¨ç¤¾äº¤åª’ä½“èˆ†æƒ…æ•°æ®åº“ä¸­æŸ¥æ‰¾çš„å…³é”®è¯ã€‚

**æ ¸å¿ƒåŸåˆ™**ï¼š
1. **è´´è¿‘ç½‘æ°‘è¯­è¨€**ï¼šä½¿ç”¨æ™®é€šç½‘å‹åœ¨ç¤¾äº¤åª’ä½“ä¸Šä¼šä½¿ç”¨çš„è¯æ±‡
2. **é¿å…ä¸“ä¸šæœ¯è¯­**ï¼šä¸ä½¿ç”¨"èˆ†æƒ…"ã€"ä¼ æ’­"ã€"å€¾å‘"ã€"å±•æœ›"ç­‰å®˜æ–¹è¯æ±‡
3. **ç®€æ´å…·ä½“**ï¼šæ¯ä¸ªå…³é”®è¯è¦éå¸¸ç®€æ´æ˜äº†ï¼Œä¾¿äºæ•°æ®åº“åŒ¹é…
4. **æƒ…æ„Ÿä¸°å¯Œ**ï¼šåŒ…å«ç½‘æ°‘å¸¸ç”¨çš„æƒ…æ„Ÿè¡¨è¾¾è¯æ±‡
5. **æ•°é‡æ§åˆ¶**ï¼šæœ€å°‘æä¾›10ä¸ªå…³é”®è¯ï¼Œæœ€å¤šæä¾›20ä¸ªå…³é”®è¯
6. **é¿å…é‡å¤**ï¼šä¸è¦è„±ç¦»åˆå§‹æŸ¥è¯¢çš„ä¸»é¢˜

**é‡è¦æé†’**ï¼šæ¯ä¸ªå…³é”®è¯éƒ½å¿…é¡»æ˜¯ä¸€ä¸ªä¸å¯åˆ†å‰²çš„ç‹¬ç«‹è¯æ¡ï¼Œä¸¥ç¦åœ¨è¯æ¡å†…éƒ¨åŒ…å«ç©ºæ ¼ã€‚ä¾‹å¦‚ï¼Œåº”ä½¿ç”¨ "é›·å†›ç­äº‰è®®" è€Œä¸æ˜¯é”™è¯¯çš„ "é›·å†›ç­ äº‰è®®"ã€‚

**è¾“å‡ºæ ¼å¼**ï¼š
è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
    "reasoning": "é€‰æ‹©è¿™äº›å…³é”®è¯çš„ç†ç”±"
}

**ç¤ºä¾‹**ï¼š
è¾“å…¥ï¼š"æ­¦æ±‰å¤§å­¦èˆ†æƒ…ç®¡ç† æœªæ¥å±•æœ› å‘å±•è¶‹åŠ¿"
è¾“å‡ºï¼š
{
    "keywords": ["æ­¦å¤§", "æ­¦æ±‰å¤§å­¦", "å­¦æ ¡ç®¡ç†", "å¤§å­¦", "æ•™è‚²"],
    "reasoning": "é€‰æ‹©'æ­¦å¤§'å’Œ'æ­¦æ±‰å¤§å­¦'ä½œä¸ºæ ¸å¿ƒè¯æ±‡ï¼Œè¿™æ˜¯ç½‘æ°‘æœ€å¸¸ä½¿ç”¨çš„ç§°å‘¼ï¼›'å­¦æ ¡ç®¡ç†'æ¯”'èˆ†æƒ…ç®¡ç†'æ›´è´´è¿‘æ—¥å¸¸è¡¨è¾¾ï¼›é¿å…ä½¿ç”¨'æœªæ¥å±•æœ›'ã€'å‘å±•è¶‹åŠ¿'ç­‰ç½‘æ°‘å¾ˆå°‘ä½¿ç”¨çš„ä¸“ä¸šæœ¯è¯­"
}"""

    def _build_user_prompt(self, original_query: str, context: str) -> str:
        """æ„å»ºç”¨æˆ·prompt"""
        prompt = f"è¯·å°†ä»¥ä¸‹æœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸ºé€‚åˆèˆ†æƒ…æ•°æ®åº“æŸ¥è¯¢çš„å…³é”®è¯ï¼š\n\nåŸå§‹æŸ¥è¯¢ï¼š{original_query}"
        
        if context:
            prompt += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}"
        
        prompt += "\n\nè¯·è®°ä½ï¼šè¦ä½¿ç”¨ç½‘æ°‘åœ¨ç¤¾äº¤åª’ä½“ä¸ŠçœŸå®ä½¿ç”¨çš„è¯æ±‡ï¼Œé¿å…å®˜æ–¹æœ¯è¯­å’Œä¸“ä¸šè¯æ±‡ã€‚"
        
        return prompt
    
    @with_graceful_retry(SEARCH_API_RETRY_CONFIG, default_return={"success": False, "error": "å…³é”®è¯ä¼˜åŒ–æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"})
    def _call_qwen_api(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨Qwen API"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
            )

            if response.choices:
                content = response.choices[0].message.content
                return {"success": True, "content": content}
            else:
                return {"success": False, "error": "APIè¿”å›æ ¼å¼å¼‚å¸¸"}
        except Exception as e:
            return {"success": False, "error": f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"}
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆå½“JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        lines = text.split('\n')
        keywords = []
        
        for line in lines:
            line = line.strip()
            # æŸ¥æ‰¾å¯èƒ½çš„å…³é”®è¯
            if 'ï¼š' in line or ':' in line:
                parts = line.split('ï¼š') if 'ï¼š' in line else line.split(':')
                if len(parts) > 1:
                    potential_keywords = parts[1].strip()
                    # å°è¯•åˆ†å‰²å…³é”®è¯
                    if 'ã€' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split('ã€')])
                    elif ',' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split(',')])
                    else:
                        keywords.append(potential_keywords)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
        if not keywords:
            # æŸ¥æ‰¾å¼•å·ä¸­çš„å†…å®¹
            import re
            quoted_content = re.findall(r'["""\'](.*?)["""\']', text)
            keywords.extend(quoted_content)
        
        # æ¸…ç†å’ŒéªŒè¯å…³é”®è¯
        cleaned_keywords = []
        for keyword in keywords[:20]:  # æœ€å¤š20ä¸ª
            keyword = keyword.strip().strip('"\'""''')
            if keyword and len(keyword) <= 20:  # åˆç†é•¿åº¦
                cleaned_keywords.append(keyword)
        
        return cleaned_keywords[:20]
    
    def _validate_keywords(self, keywords: List[str]) -> List[str]:
        """éªŒè¯å’Œæ¸…ç†å…³é”®è¯"""
        validated = []
        
        # ä¸è‰¯å…³é”®è¯ï¼ˆè¿‡äºä¸“ä¸šæˆ–å®˜æ–¹ï¼‰
        bad_keywords = {
            'æ€åº¦åˆ†æ', 'å…¬ä¼—ååº”', 'æƒ…ç»ªå€¾å‘',
            'æœªæ¥å±•æœ›', 'å‘å±•è¶‹åŠ¿', 'æˆ˜ç•¥è§„åˆ’', 'æ”¿ç­–å¯¼å‘', 'ç®¡ç†æœºåˆ¶'
        }
        
        for keyword in keywords:
            if isinstance(keyword, str):
                keyword = keyword.strip().strip('"\'""''')
                
                # åŸºæœ¬éªŒè¯
                if (keyword and 
                    len(keyword) <= 20 and 
                    len(keyword) >= 1 and
                    not any(bad_word in keyword for bad_word in bad_keywords)):
                    validated.append(keyword)
        
        return validated[:20]  # æœ€å¤šè¿”å›20ä¸ªå…³é”®è¯
    
    def _fallback_keyword_extraction(self, original_query: str) -> List[str]:
        """å¤‡ç”¨å…³é”®è¯æå–æ–¹æ¡ˆ"""
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        # ç§»é™¤å¸¸è§çš„æ— ç”¨è¯æ±‡
        stop_words = {'ã€'}
        
        # åˆ†å‰²æŸ¥è¯¢
        import re
        # æŒ‰ç©ºæ ¼ã€æ ‡ç‚¹åˆ†å‰²
        tokens = re.split(r'[\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]+', original_query)
        
        keywords = []
        for token in tokens:
            token = token.strip()
            if token and token not in stop_words and len(token) >= 2:
                keywords.append(token)
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢çš„ç¬¬ä¸€ä¸ªè¯
        if not keywords:
            first_word = original_query.split()[0] if original_query.split() else original_query
            keywords = [first_word] if first_word else ["çƒ­é—¨"]
        
        return keywords[:20]

# å…¨å±€å®ä¾‹
keyword_optimizer = KeywordOptimizer()
